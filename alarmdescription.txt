
### **CPUUtilizationAlarm**

```
High CPU utilization detected (>80%).  
This may indicate heavy query load or inefficient queries.

Resolution steps:
1. Check CloudWatch metrics for sustained CPU usage.
2. Review Performance Insights or pg_stat_activity for long-running or blocking queries.
3. Analyze slow query logs.
4. Confirm instance size fits workload; consider scaling up or adding read replicas.
5. If usage remains high >30 min, engage DBA or scaling team.
```

---

### **ReadIOPSAlarm**

```
High Read IOPS detected (>10,000).  
This suggests intensive read activity or insufficient caching.

Resolution steps:
1. Verify read traffic using CloudWatch metrics and Enhanced Monitoring.
2. Check buffer cache hit ratio (should be >99%).
3. Identify frequent queries or reports performing large reads.
4. Optimize indexing or caching at the application level.
5. Consider upgrading storage type (e.g., to provisioned IOPS) or instance class.
```

---

### **FreeableMemoryAlarm**

```
Low available memory detected (<200 MB).  
This may cause query latency or connection instability.

Resolution steps:
1. Confirm metric trend in CloudWatch; short spikes may be harmless.
2. Check active sessions and large in-memory operations.
3. Tune work_mem, shared_buffers, and autovacuum settings.
4. Restart or scale up instance if memory pressure persists.
5. Engage DBA if swap usage or memory saturation continues.
```

---

### **MemoryUtilizationAlarm**

```
High database load or memory utilization (>80%).  
May indicate resource contention from queries or cache growth.

Resolution steps:
1. Check DBLoad and CPU metrics for concurrency spikes.
2. Review pg_stat_activity for blocking or parallel queries.
3. Evaluate parameter settings like max_connections and work_mem.
4. Tune queries or scale instance vertically.
5. If sustained >30 min, escalate to DBA for workload analysis.
```

---

### **DiskUtilizationAlarm**

```
High disk queue depth or I/O wait (>80%).  
Likely caused by intensive writes, insufficient IOPS, or vacuum operations.

Resolution steps:
1. Check CloudWatch IOPS and latency metrics.
2. Review DB write activity, autovacuum jobs, or batch operations.
3. Ensure storage is not near full and IOPS capacity is adequate.
4. Consider moving to Provisioned IOPS or higher class instance.
5. If issue persists, open a storage performance investigation.
```

---

### **DatabaseConnectionsAlarm**

```
High number of active connections (>2500).  
This can lead to resource exhaustion or connection timeouts.

Resolution steps:
1. Check pg_stat_activity for idle or long-running connections.
2. Ensure application connection pooling (e.g., PgBouncer) is in use.
3. Identify sudden traffic surges or unclosed app sessions.
4. Tune max_connections and pool size if appropriate.
5. If persists >15 min, restart connection pool or scale instance.
```



